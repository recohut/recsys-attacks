{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E4i1e2FJiKb"
      },
      "source": [
        "# Attacks on Recommender Systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1yt9jiJjSbH"
      },
      "source": [
        "Recommender systems play a crucial role in helping users to find their interested information in various web services such as Amazon, YouTube, and Google News. Various recommender systems, ranging from neighborhood-based, association-rule-based, matrix-factorization-based, to deep learning based, have been developed and deployed in industry. Among them, deep learning based recommender systems become increasingly popular due to their superior performance.\n",
        "\n",
        "Recommendation Systems (RS) have become an essential part of many online services. Due to its pivotal role in guiding customers towards purchasing, there is a natural motivation for unscrupulous parties to spoof RS for profits.\n",
        "\n",
        "With the advancement of recommender systems, various techniques are employed to influence the output of recommender systems to promote or demote a particular product. Attacks are the inserting of bogus data into a recommendation system. Collaborative Filtering based Recommender Systems are the most sensitive systems to attacks in which malicious users insert fake profiles into the rating database in order to bias the systemâ€™s output (these types of attacks are known as profile injection or Shilling attacks). Purpose of the attacks can be different: to push(push attack)/decrease(nuke attack) some itemsâ€™ ratings by manipulating the recommender system, manipulation of the â€œInternet opinionâ€ or simply to sabotage the system.\n",
        "\n",
        "The attacks technique is to create numerous fake accounts / profiles and issue high or low ratings to the â€œtarget itemâ€.\n",
        "\n",
        "The general description of the profile of a true user and fake user are characterized as 80% unrated items and 20% rated items for the â€œtrueâ€ profile\" , whereas â€œfakeâ€\" profile consists of 20% unrated items and 80% rated (target items + selected items + filler items). From above description of trusted and fake user profile it is clear that to attack a recommender system, attack profile need to be designed as statistically identical to genuine profile as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjjSha-xjS4s"
      },
      "source": [
        "## Attacker's goals\n",
        "\n",
        "### Item promotion\n",
        "\n",
        "Manipulate a recommender system such that the attacker-chosen target items are recommended to many users.\n",
        "\n",
        "### Item demotion\n",
        "\n",
        "a.k.a. nuke attack. \n",
        "\n",
        "### Target specific user group\n",
        "\n",
        "Target user group is the group of users that an attack aims at.\n",
        "\n",
        "### Ancillary effects\n",
        "\n",
        "Ancillary effects (e.g., demoting competitors, bias the ratings of a special user groups on selected items) are also desired in the attack. Such intentions will manifest in choosing selected items."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrunHe_qjbHk"
      },
      "source": [
        "## Attacker's knowledge\n",
        "\n",
        "### Access to data\n",
        "\n",
        "The attack capability increase if attacker already has access to the data like user-item interaction matrix.\n",
        "\n",
        "### Access to the neural architecture\n",
        "\n",
        "Attack capability increase if attacker has access to the neural architecture to the target recommender system.\n",
        "\n",
        "## Attack types\n",
        "\n",
        "### Data poisoning\n",
        "\n",
        "a.k.a. fake data injection. Injects fake users with carefully crafted ratings to a recommender system. These fake data will be included in the training dataset of the target recommender system and then poisons the training process. In case of item promotion as a goal, these injected ratings would maximize the number of normal users to whom the target items are recommended. \n",
        "\n",
        "Recommendation engines are prone to performance alteration by malicious users that might be able to poison the training data with hand-engineered, and machine-learning optimized, fake user profiles (shilling profiles). An attacker's goal is to manipulate a recommender system such that the attacker-chosen target items are recommended to many users. To achieve this goal, the attack injects fake users with carefully crafted ratings to a recommender system.\n",
        "\n",
        "According to whether data poisoning attacks are focused on a specific type of recommender system, we can divide them into two categories: algorithm-agnostic and algorithm-specific. The former (e.g., types of shilling attacks like random attacks and bandwagon attacks) does not consider the algorithm used by the recommender system and therefore often has limited effectiveness. For instance, random attacks just choose rated items at random from the whole item set for fake users, and bandwagon attacks tend to select certain items with high popularity in the dataset for fake users. The algorithm-specific data poisoning attacks are optimized to a specific type of recommender systems and have been developed for graph-based recommender systems, association-rule-based recommender systems, matrix-factorization-based recommender systems, and neighborhood-based recommender systems.\n",
        "\n",
        "Data poisoning attacks pose severe threats to the trustworthiness of recommender systems and could manipulate Internet opinions. For instance, if an attacker manipulates a news recommender system such that a particular type of news are always recommended to users, then the attacker may be able to manipulate the usersâ€™ opinions.\n",
        "\n",
        "### Profile pollution\n",
        "\n",
        "Unlike the data poisoning attack, profile pollution attack is done at the testing time. It pollutes the historical behavior of normal users. It relies on cross-site request forgery (CSRF), and only applicable to item-to-item recommender systems.\n",
        "\n",
        "### Image spoofing\n",
        "\n",
        "In this attack, images of a category of low recommended products (e.g., socks) are perturbed to misclassify the deep neural classifier towards the class of more recommended products (e.g., running shoes) with human-level slight images alterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaPBgHybjbcS"
      },
      "source": [
        "### Evasion vs poisoning attacks\n",
        "\n",
        "<p><center><figure><img src='_images/US026046_1.png'><figcaption>A schematic representation of the distinction between evasion attacks and poisoning attacks.</figcaption></figure></center></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYHQ46IUjpSK"
      },
      "source": [
        "## Examples\n",
        "\n",
        "### Item promotion\n",
        "\n",
        "<p><center><img src='_images/US026046_2.png'></center></p>\n",
        "\n",
        "An example of a simple promotion attack favoring the target item Item6.\n",
        "\n",
        "### Item nuke\n",
        "\n",
        "<p><center><img src='_images/US026046_3.png'></center></p>\n",
        "\n",
        "An example of a simple nuke attack disfavoring the target item Item6.\n",
        "\n",
        "### False reviews\n",
        "\n",
        "Amazon products' reviews is distorted with thousands of fake ones. False reviews were helping unknown brands dominate searches for popular items. Hundreds of unverified five-star reviews were being posted on product pages in a single day. Many product pages also included positive reviews for completely different items.\n",
        "\n",
        "<p><center><img src='_images/US026046_4.png'></center></p>\n",
        "\n",
        "## Countermeasures\n",
        "\n",
        "Attack Profiles created by traditional models are effective in promoting an item, but they are highly correlated and hence can be detected by the Recommender System easily.\n",
        "\n",
        "<p><center><img src='_images/US026046_5.png'></center></p>\n",
        "\n",
        "True Profiles have huge Variance but low Covariance and in case of Fake Profiles it is vice versa.\n",
        "\n",
        "<p><center><img src='_images/US026046_6.png'></center></p>\n",
        "\n",
        "True Profiles in Green and Fake Profiles in Red Detection done using PCA.\n",
        "\n",
        "### Goal\n",
        "\n",
        "Protect something (important to the recommender or its users)\n",
        "\n",
        "- from someone\n",
        "- who has goals\n",
        "- and certain capabilities\n",
        "\n",
        "For example, `influence limiter` threat model:\n",
        "\n",
        "- protect recommender accuracy and neutrality\n",
        "- from malicious users\n",
        "- who want to push or kill products\n",
        "- and create fake accounts\n",
        "\n",
        "### Methods\n",
        "\n",
        "To reduce this risk, various detection techniques have been proposed to detect such attacks, which use diverse features extracted from user profiles. Detection Techniques can be described as some descriptive statistics that can be used to capture some of the major characteristics that make an attackerâ€™s profile look different from genuine userâ€™s profile.\n",
        "\n",
        "- Rating Deviation from Mean Agreement (RDMA) can identify attackers by analysing the profileâ€™s average deviation per item or user.\n",
        "- Weighted Deviation from Mean Agreement (WDMA) can help identify anomalies by placing a higher weight on rating deviations for sparse items.\n",
        "- Length Variance (LengthVar) is used to capture how much the length of a given profile varies from average length in the dataset. It is particularly effective in detecting attacks with large filler sizes.\n",
        "- Degree of Similarity with Top Neighbours (DegSim) is used to capture the average similarity of a profileâ€™s k nearest neighbours.\n",
        "- Increase profile injection costs (Captchas, Lowâ€cost manual insertion)\n",
        "- Use statistical attack detection methods (detect groups of users who collaborate to push/nuke items, monitor development of ratings for an item: changes in average rating, in rating entrophy; use ML to detect fake profiles).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CdohUnskdcO"
      },
      "source": [
        "## Timeline\n",
        "\n",
        "<p><center><img src='_images/US026046_7.png'></center></p>\n",
        "\n",
        "## Traditional attack methods\n",
        "\n",
        "- Random Attack: take random values for filler items, high/low ratings for target items.\n",
        "- Average Attack: attack profiles are generated such that the rating for filler items is the mean or average rating for that item across all the users in the database.\n",
        "- Bandwagon attack: profiles are generated such that besides giving high ratings to the target items, it also contains only high values for selected items and random values to some filler items .\n",
        "- Segment Attack: the segment attack model is to make inserted bots more similar to the segment market users - to push the item within the relevant community.\n",
        "- User Shifting: In these types of attacks we basically increment or decrement all ratings for a subset of items per attack profile by a constant amount so as to reduce the similarity between attack profiles.\n",
        "- Mixed Attack: In Mixed Attack, attack is on the same target item but that attack is produced from different attack modules.\n",
        "- Noise Injection: This type of attack is carried out by adding some noise to ratings according to a standard normal distribution multiplied by a constant, Î², which is used to govern the amount of noise to be added."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05mDbNFGkmws"
      },
      "source": [
        "```{tableofcontents}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3Ip8icUkgKE"
      },
      "source": [
        "## References\n",
        "\n",
        "1. Profile Injection Attack Detection for Securing Collaborative Recommender Systems. [Chad Williams. 2006.](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.219.2864&rep=rep1&type=pdf)\n",
        "2. Defending Recommender Systems: Detection of Profile Injection Attacks. [Chad A. Williams, Bamshad Mobasher, and Robin Burke. 2007.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.122.8693&rep=rep1&type=pdf)\n",
        "3. Detection of Profile Injection Attacks in Social Recommender Systems Using Outlier Analysis. [Anahita Davoudi and Mainak Chatterjee. 2017. IEEE.](http://eecs.ucf.edu/~anahita/08258235.pdf)\n",
        "4. Profile Injection Attack Detection in Recommender System. [Ashish Kumar (2015) Profile Injection Attack Detection in Recommender System [Master Thesis]](https://gdeepak.com/thesisme/Thesis-Ashish.pdf)\n",
        "5. Practical Data Poisoning Attack against Next-Item Recommendation. [Hengtong Zhang, Yaliang Li, Bolin Ding, Jing Gao. 2020. arXiv.](https://arxiv.org/abs/2004.03728)\n",
        "6. Adversarial Item Promotion: Vulnerabilities at the Core of Top-N Recommenders that Use Images to Address Cold Start. [Zhuoran Liu, Martha Larson. 2020. arXiv.](https://arxiv.org/abs/2006.01888) [Zhuoran Liu (2021) AIP: Adversarial Item Promotion [Source code]](https://github.com/liuzrcc/AIP)\n",
        "7. Black-Box Attacks on Sequential Recommenders via Data-Free Model Extraction. [Zhenrui Yue, Zhankui He, Huimin Zeng, Julian McAuley. 2021. arXiv.](https://arxiv.org/abs/2109.01165) [Zhenrui (2020) PyTorch Implementation of Black-Box Attacks on Sequential Recommenders via Data-Free Model Extraction [Source code]](https://github.com/yueeeeeeee/recsys-extraction-attack)\n",
        "8. Membership Inference Attacks Against Recommender Systems. [Minxing Zhang, Zhaochun Ren, Zihan Wang, Pengjie Ren, Zhumin Chen, Pengfei Hu, Yang Zhang. 2021. arXiv.](https://arxiv.org/abs/2109.08045)\n",
        "9. A Study of Defensive Methods to Protect Visual Recommendation Against Adversarial Manipulation of Images. [http://sisinflab.poliba.it/publications/2021/ADDMM21/SIGIR2021_A_Study_of_Defensive_Methods_to_Protect_Visual_Recommendation_Against_Adversarial_Manipulation_of_Images.pdf](http://sisinflab.poliba.it/publications/2021/ADDMM21/SIGIR2021_A_Study_of_Defensive_Methods_to_Protect_Visual_Recommendation_Against_Adversarial_Manipulation_of_Images.pdf). [https://github.com/sisinflab/Visual-Adversarial-Recommendation](https://github.com/sisinflab/Visual-Adversarial-Recommendation)\n",
        "10. PoisonRec: An Adaptive Data Poisoning Framework for Attacking Black-box Recommender Systems. [Junshuai Song, Zhao Li, Zehong Hu, Yucheng Wu, Zhenpeng Li, Jian Li and Jun Gao. 2020. IEEE.](https://conferences.computer.org/icde/2020/pdfs/ICDE2020-5acyuqhpJ6L9P042wmjY1p/290300a157/290300a157.pdf)\n",
        "11. Ready for Emerging Threats to Recommender Systems? A Graph Convolution-based Generative Shilling Attack. [Fan Wu, Min Gao, Junliang Yu, Zongwei Wang, Kecheng Liu, Xu Wange. 2021. arXiv.](https://arxiv.org/abs/2107.10457)\n",
        "12. A Black-Box Attack Model for Visually-Aware Recommender Systems. [Rami Cohen, Oren Sar Shalom, Dietmar Jannach, Amihood Amir. 2020. arXiv.](https://arxiv.org/abs/2011.02701) [https://github.com/vis-rs-attack/code](https://github.com/vis-rs-attack/code)\n",
        "13. Poisoning Attack against Estimating from Pairwise Comparisons. [Ke Ma, Qianqian Xu, Jinshan Zeng, Xiaochun Cao, and Qingming Huang. 2021. arXiv.](https://arxiv.org/abs/2107.01854v1) [alphaprime (2021) Poisoning Attack against Estimating from Pairwise Comparisons [Source code]](https://github.com/alphaprime/Poisonging_Attack_Pairwise_Comparison)\n",
        "14. Assessing Perceptual and Recommendation Mutation of Adversarially-Poisoned Visual Recommenders. [Paper](http://sisinflab.poliba.it/publications/2020/ADMM20/CR_WDCS_NeurIPS2020_Assessing_Perceptual_and_Recommendation_Mutation_of_Adversarialli_Poisoned_Visual_Recommenders.pdf). [Code](https://github.com/sisinflab/adversarial-recommender-systems-survey/blob/master/Perceptual-Rec-Mutation-of-Adv-VRs)\n",
        "15. Multi-Step Adversarial Perturbations on Recommender Systems Embeddings. [Paper](https://arxiv.org/abs/2010.01329). [https://anonymous.4open.science/r/9f27f909-93d5-4016-b01c-8976b8c14bc5/](https://www.notion.so/9f27f90993d54016b01c8976b8c14bc5)\n",
        "16. Adversarial Training Towards Robust Multimedia Recommender System. [Paper](https://ieeexplore.ieee.org/document/8618394). [Code](https://github.com/duxy-me/AMR)\n",
        "17. A survey on Adversarial Recommender Systems: from Attack/Defense strategies to Generative Adversarial Networks. [Yashar Deldjoo, Tommaso Di Noia, Felice Antonio Merra. 2021. arXiv.](https://arxiv.org/abs/2005.10322) [**https://github.com/sisinflab/adversarial-recommender-systems-survey**](https://github.com/sisinflab/adversarial-recommender-systems-survey)\n",
        "18. A Complete List of All (arXiv) Adversarial Example PapersÂ [ğŸŒLink](https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html)\n",
        "19. Graph Adversarial Learning LiteratureÂ [Link](https://github.com/safe-graph/graph-adversarial-learning-literature)\n",
        "20. Awesome Graph Adversarial LearningÂ [Link](https://github.com/gitgiter/Graph-Adversarial-Learning)\n",
        "21. Awesome Graph Attack and Defense PapersÂ [Link](https://github.com/ChandlerBang/awesome-graph-attack-papers)\n",
        "22. Segment-Focused Shilling Attacks against Recommendation Algorithms in Binary Ratings-based Recommender Systems,Â *International Journal of Hybrid Information Technology*,Â [ğŸ“Paper](https://www.semanticscholar.org/paper/Segment-Focused-Shilling-Attacks-against-Algorithms-Zhang/5c7e96dcaf253f37904f91fdb6fdd6f486dba134)\n",
        "23. Shilling attack models in recommender system,Â *International Conference on Inventive Computation Technologies (ICICT)*,Â [ğŸ“Paper](https://ieeexplore.ieee.org/document/7824865)\n",
        "24. Graph Embedding for Recommendation against Attribute Inference Attacks,Â *WWW*,Â [ğŸ“Paper](https://arxiv.org/pdf/2101.12549.pdf)\n",
        "25. Understanding the Effects of Adversarial Personalized Ranking Optimization Method on Recommendation Quality,Â *Arxiv*,Â ğŸ“[Paper](https://arxiv.org/abs/2107.13876)\n",
        "26. GCN-Based User Representation Learning for Unifying Robust Recommendation and Fraudster Detection,Â *Arxiv*,Â [ğŸ“Paper](https://arxiv.org/abs/2005.10150)\n",
        "27. On Detecting Data Pollution Attacks On Recommender Systems Using Sequential GANs,Â *ICML*,Â [ğŸ“Paper](https://arxiv.org/abs/2012.02509)\n",
        "28. A Robust Hierarchical Graph Convolutional Network Model for Collaborative Filtering,Â *Arxiv*,Â [ğŸ“Paper](https://arxiv.org/abs/2004.14734)\n",
        "29. Adversarial Collaborative Auto-encoder for Top-N Recommendation,Â *Arxiv*,Â [ğŸ“Paper](https://arxiv.org/abs/1808.05361)\n",
        "30. Adversarial Attacks and Detection on Reinforcement Learning-Based Interactive Recommender Systems,Â *Arxiv*,Â [ğŸ“Paper](https://arxiv.org/abs/2006.07934)\n",
        "31. Adversarial Learning to Compare: Self-Attentive Prospective Customer Recommendation in Location based Social Networks,Â *WSDM*,Â [ğŸ“Paper](https://dl.acm.org/doi/abs/10.1145/3336191.3371841)\n",
        "32. Certifiable Robustness to Discrete Adversarial Perturbations for Factorization Machines,Â *SIGIR*,Â [ğŸ“Paper](http://jiyang3.web.engr.illinois.edu/files/fm-rt.pdf)\n",
        "33. Directional Adversarial Training for Recommender Systems,Â *ECAI*,Â [ğŸ“Paper](http://ecai2020.eu/papers/300_paper.pdf)\n",
        "34. Shilling Attack Detection Scheme in Collaborative Filtering Recommendation System Based on Recurrent Neural Network,Â *Future of Information and Communication Conference*,Â [ğŸ“Paper](https://link.springer.com/chapter/10.1007/978-3-030-39445-5_46)\n",
        "35. Learning Product Rankings Robust to Fake Usersï¼ŒÂ *Arxiv*,Â [ğŸ“Paper](https://arxiv.org/abs/2009.05138)\n",
        "36. Privacy-Aware Recommendation with Private-Attribute Protection using Adversarial Learning,Â *WSDM*,Â [ğŸ“Paper](https://arxiv.org/abs/1911.09872)\n",
        "37. Quick and accurate attack detection in recommender systems through user attributes,Â *RecSys*,Â [ğŸ“Paper](https://dl.acm.org/doi/10.1145/3298689.3347050)\n",
        "38. Global and Local Differential Privacy for Collaborative Bandits,Â *RecSys*,Â [ğŸ“Paper](https://dl.acm.org/doi/pdf/10.1145/3383313.3412254)\n",
        "39. Towards Safety and Sustainability: Designing Local Recommendations for Post-pandemic World,Â *RecSys*,Â [ğŸ“Paper](https://dl.acm.org/doi/pdf/10.1145/3383313.3412251)\n",
        "40. GCN-Based User Representation Learning for Unifying Robust Recommendation and Fraudster Detection,Â *RecSys*,Â [ğŸ“Paper](https://dl.acm.org/doi/abs/10.1145/3397271.3401165)\n",
        "41. Adversarial Training Towards Robust Multimedia Recommender System,Â *TKDE*,Â [ğŸ“Paper](https://graphreason.github.io/papers/35.pdf),Â [Code](https://github.com/duxy-me/AMR)\n",
        "42. Adversarial Collaborative Neural Network for Robust Recommendation,Â *SIGIR*,Â [ğŸ“Paper](https://www.researchgate.net/publication/332861957_Adversarial_Collaborative_Neural_Network_for_Robust_Recommendation)\n",
        "43. Adversarial Mahalanobis Distance-based Attentive Song Recommender for Automatic Playlist Continuation,Â *SIGIR*,Â [ğŸ“Paper](http://web.cs.wpi.edu/~kmlee/pubs/tran19sigir.pdf),Â [Code](https://github.com/thanhdtran/MASR)\n",
        "44. Adversarial tensor factorization for context-aware recommendation,Â *RecSys*,Â [ğŸ“Paper](https://dl.acm.org/doi/10.1145/3298689.3346987), [Code]\n",
        "45. Adversarial Training-Based Mean Bayesian Personalized Ranking for Recommender System,Â *IEEE Access*,Â [ğŸ“Paper](https://ieeexplore.ieee.org/document/8946325)\n",
        "46. Securing the Deep Fraud Detector in Large-Scale E-Commerce Platform via Adversarial Machine Learning Approachï¼Œ*WWW*,Â [ğŸ“Paper](https://www.ntu.edu.sg/home/boan/papers/WWW19.pdf)\n",
        "47. Shilling Attack Detection in Recommender System Using PCA and SVM,Â *Emerging technologies in data mining and information security*,Â [ğŸ“Paper](https://link.springer.com/chapter/10.1007/978-981-13-1498-8_55)\n",
        "48. Adversarial Personalized Ranking for Recommendation,Â *SIGIR*,Â [ğŸ“Paper](https://dl.acm.org/citation.cfm?id=3209981),Â [Code](https://github.com/hexiangnan/adversarial_personalized_ranking)\n",
        "49. A shilling attack detector based on convolutional neural network for collaborative recommender system in social aware network,Â *The Computer Journal*,Â [ğŸ“Paper](https://academic.oup.com/comjnl/article-abstract/61/7/949/4835634)\n",
        "50. Adversarial Sampling and Training for Semi-Supervised Information Retrieval,Â *WWW*,Â [ğŸ“Paper](https://arxiv.org/abs/1506.05752)\n",
        "51. Enhancing the Robustness of Neural Collaborative Filtering Systems Under Malicious Attacks,Â *IEEE Transactions on Multimedia*,Â [ğŸ“Paper](https://ieeexplore.ieee.org/document/8576563)\n",
        "52. An Obfuscated Attack Detection Approach for Collaborative Recommender Systems,Â *Journal of computing and information technology*,Â [ğŸ“Paper](https://hrcak.srce.hr/203982)\n",
        "53. Detecting Abnormal Profiles in Collaborative Filtering Recommender Systems,Â *Journal of Intelligent Information Systems*,Â [ğŸ“Paper](https://link.springer.com/article/10.1007/s10844-016-0424-5)\n",
        "54. Detection of Proï¬le Injection Attacks in Social Recommender Systems Using Outlier Analysis,Â *IEEE Big Data*,Â [ğŸ“Paper](http://www.cs.ucf.edu/~anahita/08258235.pdf)\n",
        "55. Prevention of shilling attack in recommender systems using discrete wavelet transform and support vector machine,Â *Eighth International Conference on Advanced Computing (ICoAC)*,Â [ğŸ“Paper](https://ieeexplore.ieee.org/document/7951753)\n",
        "56. Discovering shilling groups in a real e-commerce platform,Â *Online Information Review*,Â [ğŸ“Paper](https://www.emerald.com/insight/content/doi/10.1108/OIR-03-2015-0073/full/html)\n",
        "57. Shilling attack detection in collaborative filtering recommender system by PCA detection and perturbation,Â *International Conference on Wavelet Analysis and Pattern Recognition (ICWAPR)*,Â [ğŸ“Paper](https://ieeexplore.ieee.org/document/7731644)\n",
        "58. Re-scale AdaBoost for attack detection in collaborative filtering recommender systems,Â *KBS*,Â [ğŸ“Paper](https://www.sciencedirect.com/science/article/pii/S0950705116000861)\n",
        "59. SVM-TIA a shilling attack detection method based on SVM and target item analysis in recommender systems,Â *Neurocomputing*,Â [ğŸ“Paper](https://www.sciencedirect.com/science/article/abs/pii/S0925231216306038)\n",
        "60. Adversarial Machine Learning in Recommender Systems: State of the art and Challenges,Â *Arxiv2020*,Â [ğŸ“Paper](https://arxiv.org/abs/2005.10322)\n",
        "61. A Survey of Adversarial Learning on Graphs,Â *Arxiv2020*,Â [ğŸ“Paper](https://arxiv.org/abs/2003.05730)\n",
        "62. Adversarial Attacks and Defenses on Graphs: A Review and Empirical Study,Â *Arxiv2020*,Â [ğŸ“Paper](https://arxiv.org/abs/2003.00653)\n",
        "63. Shilling attacks against collaborative recommender systems: a review,Â *Artificial Intelligence Review*,Â [ğŸ“Paper](https://link.springer.com/article/10.1007/s10462-018-9655-x)\n",
        "64. Adversarial Attacks and Defenses in Images, Graphs and Text: A Review,Â *Arxiv2019*,Â [ğŸ“Paper](https://arxiv.org/abs/1909.08072)\n",
        "65. A Survey of Attacks in Collaborative Recommender Systems,Â *Journal of Computational and Theoretical Nanoscience 2019*,Â [ğŸ“Paper](https://www.ingentaconnect.com/content/asp/jctn/2019/00000016/f0020005/art00029)\n",
        "66. Adversarial Attack and Defense on Graph Data: A Survey,Â *Arxiv2018*,Â [ğŸ“Paper](https://arxiv.org/abs/1812.10528)\n",
        "67. Adversarial Machine Learning: The Case of Recommendation Systems,Â *IEEE 19th International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)*,Â [ğŸ“Paper](https://ieeexplore.ieee.org/abstract/document/8445767)\n",
        "68. Recommender Systems: Attack Types and Strategies,Â *AAAI*2005,Â ğŸ“[Paper](https://www.aaai.org/Papers/AAAI/2005/AAAI05-053.pdf)\n",
        "69. A Review of Attacks and Its Detection Attributes on Collaborative Recommender Systems,Â *IJARCS2017*,Â ğŸ“[Paper](http://www.ijarcs.info/index.php/Ijarcs/article/download/4550/4100)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "US026046_Attacks_on_Recommender_Systems.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
